# ğŸ¥ LipGANs: Text-to-Viseme GAN Framework

![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange?logo=tensorflow)
![Keras](https://img.shields.io/badge/Keras-red?logo=keras)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Dataset: TCD--TIMIT](https://img.shields.io/badge/Dataset-TCD--TIMIT-lightgrey)

LipGANs is a **text-to-lip animation framework** that generates short video clips of **mouth movements directly from text**, without requiring any audio input.

This project bridges **natural language processing (text â†’ phonemes)** and **computer vision (GAN-based video synthesis)** to create realistic lip articulations from scratch.

---

## ğŸš€ Features

- **Audio-free lip generation** â†’ Converts raw text directly into viseme-based animations.  
- **Phoneme-to-Viseme Mapping** â†’ Maps linguistic units to 10 distinct mouth shapes.  
- **Per-Viseme GAN Training** â†’ A separate 3D Convolutional GAN is trained for each viseme class.  
- **Dataset Preprocessing** â†’ Automatic segmentation, lip region extraction, and normalization.  
- **Smooth Video Synthesis** â†’ Concatenates generated viseme clips with temporal blending.  
- **Built on TCD-TIMIT dataset** â†’ Aligned audiovisual dataset for speech-driven lip synthesis.  

---

## ğŸ“‚ Repository Structure

```bash
LipGANs/
â”‚â”€â”€ data/                   # Preprocessed dataset (organized by viseme)
â”‚   â”œâ”€â”€ raw/                # Original TCD-TIMIT dataset (not included)
â”‚   â”œâ”€â”€ viseme_01_Closed_Lips/
â”‚   â”œâ”€â”€ viseme_02_Teeth_Touching/
â”‚   â””â”€â”€ ...
â”‚
â”‚â”€â”€ models/                 # Saved GAN models per viseme
â”‚   â”œâ”€â”€ viseme_01/
â”‚   â””â”€â”€ ...
â”‚
â”‚â”€â”€ results/                # Generated outputs & evaluation samples
â”‚
â”‚â”€â”€ src/                    # Core source code
â”‚   â”œâ”€â”€ preprocessing/      # Dataset preprocessing scripts
â”‚   â”‚   â”œâ”€â”€ phoneme_segmentation.py
â”‚   â”‚   â”œâ”€â”€ roi_extraction.py
â”‚   â”‚   â””â”€â”€ viseme_mapping.json
â”‚   â”‚
â”‚   â”œâ”€â”€ training/           # GAN training code
â”‚   â”‚   â”œâ”€â”€ gan_model.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/          # Text-to-animation pipeline
â”‚   â”‚   â”œâ”€â”€ text_to_viseme.py
â”‚   â”‚   â”œâ”€â”€ generate.py
â”‚   â”‚   â””â”€â”€ smoothing.py
â”‚
â”‚â”€â”€ requirements.txt        # Python dependencies
â”‚â”€â”€ README.md               # Project documentation
```

---

## âš™ï¸ Installation

### 1. Clone the repository
```bash
git clone https://github.com/your-username/lipgans.git
cd lipgans
```

### 2. Create a virtual environment (recommended)
```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

**Dependencies include:**
- TensorFlow / Keras  
- NumPy, OpenCV, Imageio  
- MediaPipe (for lip landmark detection)  
- ffmpeg (for slicing & assembling clips)  

---

## ğŸ“Š Dataset Setup (TCD-TIMIT)

1. **Download TCD-TIMIT** dataset manually:  
   [TCD-TIMIT Dataset](https://sigmedia.tcd.ie/tcd_timit_db)  

2. Place it under:  
   ```bash
   data/raw/
   ```

3. Run preprocessing scripts:  
   ```bash
   python src/preprocessing/phoneme_segmentation.py
   python src/preprocessing/roi_extraction.py
   ```

This will:
- Segment videos into **phoneme-aligned clips**.  
- Extract **mouth regions** using MediaPipe FaceMesh.  
- Map **phonemes â†’ visemes (10 classes)**.  
- Save **normalized 3-frame 64Ã—64 sequences** into `data/viseme_xx/`.  

---

## ğŸ—£ What are Visemes?

A **viseme** is any of several speech sounds that **look the same on the lips**, for example when lip reading.  
Unlike **phonemes** (the smallest units of sound in language), **visemes represent groups of phonemes that appear visually identical** on the face when spoken.

ğŸ‘‰ Example:
- The phonemes `/p/`, `/b/`, and `/m/` all map to the same viseme (closed lips).

This is why phoneme-to-viseme mapping is essential for lip animation:
- It reduces complexity.  
- It ensures natural-looking articulation.  

ğŸ“Œ Example mapping (simplified):

| Viseme Class        | Example Phonemes | Lip Shape Description       |
|---------------------|------------------|-----------------------------|
| Closed Lips         | /p/, /b/, /m/    | Lips fully closed           |
| Teeth Touching      | /t/, /d/         | Tongue touches teeth        |
| Open Mouth (wide)   | /a/, /aa/        | Jaw dropped, lips open wide |
| Rounded Lips        | /oo/, /uw/, /w/  | Lips rounded forward        |

---

## ğŸ‹ï¸ Training

Train a GAN for a specific viseme class:  

```bash
python src/training/train.py --viseme_id 03 --epochs 200
```

- `--viseme_id`: Viseme class (01â€“10).  
- `--epochs`: Number of training epochs (default = 200).  

Trained models will be stored in:  
```
models/viseme_xx/
```

---

## ğŸ¬ Inference (Text â†’ Animation)

> âš ï¸ **Note:** The raw output of LipGANs is a **sequence of generated frames** (per viseme) for maximum clarity. These frames can then be concatenated into an animation video (MP4/GIF) if needed.

Generate a lip animation for any input text:  

```bash
python src/inference/generate.py --text "Hello world"
```

**Steps performed:**  
1. **Text â†’ Phonemes** (using CMU Pronouncing Dictionary).  
2. **Phonemes â†’ Visemes** (via `viseme_mapping.json`).  
3. **GAN Generation**: Loads each viseme GAN and generates 3-frame clips.  
4. **Chaining & Smoothing**: Concatenates clips with temporal blending.  

Output saved in:  
```
results/hello_world.mp4
```

ğŸ“Œ Example:  
```bash
python src/inference/generate.py --text "Good morning"
```

Output â†’ `results/good_morning.mp4`  

---

## ğŸ“ˆ Results

| Approach | Output Quality |
|----------|----------------|
| Single Multi-Class GAN | Blurry, frequent mode collapse |
| Per-Viseme GANs (ours) | Sharper details, stable articulation |

âœ… Generated clips show **accurate viseme realization** and **plausible articulation** across unseen speakers.  

---

## ğŸŒ Applications

- ğŸ­ **Virtual Avatars & Chatbots** â†’ Realistic mouth articulation in animated characters.  
- ğŸ—£ **Speech Therapy Tools** â†’ Helping learners visualize correct articulation.  
- ğŸ¦» **Assistive Technology for the Deaf/Hard of Hearing** â†’  
  Deaf children (or learners with hearing difficulties) can simply **type a word/sentence into the UI** and see a **sequence of lip movements (frames or animation)** showing how it would be spoken. This bridges the gap between written text and spoken articulation.  
- ğŸ® **Gaming & AR/VR** â†’ Lifelike lip-syncing for immersive experiences. Can be used by animated characters
- ğŸ¬ **Audio Dubbing & Localization** â†’ Generate realistic lip movements that match translated text for films, shows, and animations.

---

## ğŸ”® Roadmap

- ğŸ”¹ **Speaker-conditioned GANs** (identity preservation).  
- ğŸ”¹ **Variable-length viseme clips** for realistic timing.  
- ğŸ”¹ **Quantitative evaluation** using FVD, lip-reading accuracy.  
- ğŸ”¹ **Multilingual support** (phoneme mappings for other languages).  
- ğŸ”¹ **Real-time integration** for virtual avatars and chatbots.
- ğŸ”¹ **Integration with dubbing & localization pipelines** for film and media industries.  

---

## ğŸ¤ Contributing

Contributions are welcome!  
- Fork the repo  
- Create a new branch (`feature-xyz`)  
- Commit your changes  
- Open a Pull Request ğŸš€  

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€“ see [LICENSE](LICENSE) for details.  

---

## ğŸ”— Citation

If you use this project in your research, please cite:  

```bibtex
@misc{lipgans2025,
  author = {Nandita Singh},
  title = {LipGANs: Text-to-Viseme GAN Framework for Audio-Free Lip Animation Generation},
  year = {2025},
  url = {https://github.com/madebynanditaaa/lipgans}
}
```

---

âœ¨ With **LipGANs**, we take the first step towards **speech-free, text-driven lip animation** for next-gen humanâ€“computer interaction and accessibility!  
