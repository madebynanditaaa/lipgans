#  LipGANS: Text-to-Viseme GAN Framework

![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange?logo=tensorflow)
![Keras](https://img.shields.io/badge/Keras-red?logo=keras)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Dataset: TCD--TIMIT](https://img.shields.io/badge/Dataset-TCD--TIMIT-lightgrey)

**LipGANS** is a **one-of-its-kind text-to-lip animation framework** that generates short video clips of **mouth movements directly from text**, without requiring any audio input.  

Unlike audio-driven lip-sync systems â€” where phoneme durations are known from speech timing â€” **LipGANs must predict phoneme durations from text alone**, making the task significantly more challenging and unique.  

It combines **natural language processing** (text â†’ phonemes) and **computer vision** (GAN-based video synthesis) to create realistic lip articulations from scratch.  

---

### ğŸ”„ Pipeline

**Text â†’ Phonemes â†’ Predicted Durations â†’ Visemes â†’ GANs â†’ Frames â†’ Video**

---

## ğŸš€ Features

- **Audio-free lip generation** â†’ Converts raw text directly into viseme-based animations.  
- **Phoneme-to-Viseme Mapping** â†’ Maps linguistic units to 10 distinct mouth shapes.  
- **Per-Viseme GAN Training** â†’ A separate 3D Convolutional GAN is trained for each viseme class.
- Automatic Dataset Preprocessing â†’ Segmentation, lip ROI extraction, normalization. 
- **Built on TCD-TIMIT dataset** â†’ Aligned audiovisual dataset for speech-driven lip synthesis.  

---

## ğŸ“‚ Repository Structure

```bash
lipgans/
â”œâ”€ README.md                # Project documentation
â”œâ”€ requirements.txt         # Python dependencies
â”œâ”€ .gitignore               # Git ignore rules
â”œâ”€ config/
â”‚   â””â”€ paths.example.yaml   # Example YAML for setting dataset and model paths
â”œâ”€ src/
â”‚   â””â”€ lipgans/
â”‚       â”œâ”€ __init__.py
â”‚       â”œâ”€ config.py            # Config options: paths, latent dims, FPS, frame size
â”‚       â”œâ”€ phonemes.py          # Functions to convert word â†’ phonemes â†’ visemes
â”‚       â”œâ”€ data/                # Dataset preprocessing utilities
â”‚       â”‚   â”œâ”€ mlf_parser.py         # Parses TCD-TIMIT phoneme MLF files
â”‚       â”‚   â”œâ”€ extract_viseme_clips.py # Segments video/audio into per-viseme clips
â”‚       â”‚   â”œâ”€ crop_mouth.py         # Crops mouth ROI from frames
â”‚       â”‚   â””â”€ dataset.py            # Dataset helper: load & organize clips for GAN training
â”‚       â”œâ”€ models/
â”‚       â”‚   â””â”€ gan3d.py             # 3D convolutional GAN architecture per viseme
â”‚       â”œâ”€ train/
â”‚       â”‚   â””â”€ train_viseme.py      # Script to train a single viseme GAN
â”‚       â”œâ”€ generate/
â”‚       â”‚   â”œâ”€ merge_gans.py        # Load per-viseme GANs, generate frames, save PNG/GIF/MP4
â”‚       â”‚   â””â”€ frontend.py          # Optional GUI / interface to generate words interactively
â”‚       â””â”€ utils/
â”‚           â”œâ”€ io.py                # File I/O helpers
â”‚           â”œâ”€ video.py             # Video assembling & frame handling helpers
â”‚           â””â”€ seed.py              # Random seed initialization for reproducibility
â”œâ”€ scripts/                     # High-level scripts for batch processing or experiments
â”‚   â”œâ”€ extract_all.py           # Slice all videos into per-viseme clips
â”‚   â”œâ”€ crop_all.py              # Crop mouth regions for all dataset videos
â”‚   â”œâ”€ train_all.py             # Train GANs for all viseme classes
â”‚   â”œâ”€ generate_word.py         # Generate lip animation for a single word
â”‚   â””â”€ preview_crops.py         # Quick preview of cropped mouth ROIs
â””â”€ examples/                     # Example outputs
    â””â”€ demo_words.txt            # List of example words for demo generation

```

---

## âš™ï¸ Installation

### 1. Clone the repository
```bash
git clone https://github.com/your-username/lipgans.git
cd lipgans
```

### 2. Create a virtual environment (recommended)
```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

**Dependencies include:**
- TensorFlow / Keras  
- NumPy, OpenCV, Imageio  
- MediaPipe (for lip landmark detection)  
- ffmpeg (for slicing & assembling clips)
- NLTK (for CMU Pronouncing Dictionary)
---

## ğŸ“Š Dataset Setup (TCD-TIMIT)

1. **Download TCD-TIMIT** dataset manually:  
   [TCD-TIMIT Dataset](https://sigmedia.tcd.ie/tcd_timit_db)  

2. Place it under:  
   ```bash
   data/raw/
   ```

3. Run preprocessing scripts:  
   ```bash
   python src/lipgans/data/extract_viseme_clips.py
   python src/lipgans/data/crop_mouth.py
   
   ```

This will:
- Segment videos into **phoneme-aligned clips**.  
- Extract **mouth regions** using MediaPipe FaceMesh.  
- Map **phonemes â†’ visemes (10 classes)**.  
- Save **normalized 3-frame 64Ã—64 sequences** into `data/viseme_xx/`.  

---

## ğŸ—£ What are Visemes?

A **viseme** is any of several speech sounds that **look the same on the lips**, for example when lip reading.  
Unlike **phonemes** (the smallest units of sound in language), **visemes represent groups of phonemes that appear visually identical** on the face when spoken.

ğŸ‘‰ Example:
- The phonemes `/p/`, `/b/`, and `/m/` all map to the same viseme (closed lips).

This is why phoneme-to-viseme mapping is essential for lip animation:
- It reduces complexity.  
- It ensures natural-looking articulation.  

ğŸ“Œ Example mapping (simplified):

| Viseme Class        | Example Phonemes | Lip Shape Description       |
|---------------------|------------------|-----------------------------|
| Closed Lips         | /p/, /b/, /m/    | Lips fully closed           |
| Teeth Touching      | /t/, /d/         | Tongue touches teeth        |
| Open Mouth (wide)   | /a/, /aa/        | Jaw dropped, lips open wide |
| Rounded Lips        | /oo/, /uw/, /w/  | Lips rounded forward        |

---

## ğŸ‹ï¸ Training

Train a GAN for a specific viseme class:  

```bash
python src/training/train.py --viseme_id 03 --epochs 200
```

- `--viseme_id`: Viseme class (01â€“10).  
- `--epochs`: Number of training epochs (default = 200).  

Trained models will be stored in:  
```
models/viseme_xx/
```

---

## ğŸ¬ Inference (Text â†’ Animation)

The output is a sequence of generated frames (PNG), which can also be saved as GIF or MP4.

```bash
python src/lipgans/generate/generate_word.py
```

**Steps performed:**  
1. **Text â†’ Phonemes** (using CMU Pronouncing Dictionary).  
2. **Phonemes â†’ Visemes** (via `viseme_mapping.json`).  
3. **GAN Generation**: Loads each viseme GAN and generates 3-frame clips.  
4. **Chaining & Smoothing**: Concatenates clips with temporal blending.  

Output saved in:  
```
example/cat/
 â”œâ”€ cat_01.png
 â”œâ”€ cat_02.png
 â”œâ”€ cat_03.png
 â”œâ”€ ...
 â”œâ”€ cat.gif
 â””â”€ cat.mp4

```
---

## ğŸ“ˆ Results

| Approach | Output Quality |
|----------|----------------|
| Single Multi-Class GAN | Blurry, frequent mode collapse |
| Per-Viseme GANs (ours) | Sharper details, stable articulation |

âœ… Generated clips show **accurate viseme realization** and **plausible articulation** across unseen speakers.  

---

## ğŸŒ Applications

- ğŸ­ **Virtual Avatars & Chatbots** â†’ Realistic mouth articulation in animated characters.  
- ğŸ—£ **Speech Therapy Tools** â†’ Helping learners visualize correct articulation.  
- ğŸ¦» **Assistive Technology for the Deaf/Hard of Hearing** â†’  
  Deaf children (or learners with hearing difficulties) can simply **type a word/sentence into the UI** and see a **sequence of lip movements (frames or animation)** showing how it would be spoken. This bridges the gap between written text and spoken articulation.  
- ğŸ® **Gaming & AR/VR** â†’ Lifelike lip-syncing for immersive experiences. Can be used by animated characters
- ğŸ¬ **Audio Dubbing & Localization** â†’ Generate realistic lip movements that match translated text for films, shows, and animations.

---

## ğŸ”® Roadmap

- ğŸ”¹ **Speaker-conditioned GANs** (identity preservation).  
- ğŸ”¹ **Variable-length viseme clips** for realistic timing.  
- ğŸ”¹ **Quantitative evaluation** using FVD, lip-reading accuracy.  
- ğŸ”¹ **Multilingual support** (phoneme mappings for other languages).  
- ğŸ”¹ **Real-time integration** for virtual avatars and chatbots.
- ğŸ”¹ **Integration with dubbing & localization pipelines** for film and media industries.  

---

## ğŸ¤ Contributing

Contributions are welcome!  
- Fork the repo  
- Create a new branch (`feature-xyz`)  
- Commit your changes  
- Open a Pull Request ğŸš€  

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€“ see [LICENSE](LICENSE) for details.  

---

## ğŸ”— Citation

If you use this project in your research, please cite:  

```bibtex
@misc{lipgans2025,
  author = {Nandita Singh},
  title = {LipGANs: Text-to-Viseme GAN Framework for Audio-Free Lip Animation Generation},
  year = {2025},
  url = {https://github.com/madebynanditaaa/lipgans}
}
```

---

âœ¨ With **LipGANs**, we take the first step towards **speech-free, text-driven lip animation** for next-gen humanâ€“computer interaction and accessibility!  